/**
 * Migration Workers for Background Processing
 * Handles GoTeamUp data migration tasks using BullMQ
 */

import { Job } from "bullmq";
import { enhancedQueueManager } from "../enhanced-queue-manager";
import { migrationService } from "../../services/migration-service";
import { analyzeDataWithAI } from "../../services/ai-data-analyzer";
import { supabaseAdmin } from "../../supabase/admin";

export interface MigrationJobData {
  migrationJobId: string;
  organizationId: string;
  analysisType?: string;
  batchSize?: number;
  skipDuplicates?: boolean;
}

export interface ImportJobData extends MigrationJobData {
  recordIds: string[];
  targetTable: string;
  mappingId: string;
}

export interface ConflictResolutionData extends MigrationJobData {
  conflictIds: string[];
  resolution: {
    action: "use_existing" | "use_new" | "merge" | "skip";
    data?: any;
  };
}

/**
 * Initialize all migration workers
 */
export async function initializeMigrationWorkers(): Promise<void> {
  console.log("üîß Initializing migration workers...");

  try {
    // Register workers for different migration tasks
    await Promise.all([
      enhancedQueueManager.registerWorker(
        "AI_PROCESSING",
        processMigrationAnalysis,
      ),
      enhancedQueueManager.registerWorker(
        "WORKFLOW_ACTIONS",
        processMigrationImport,
      ),
      enhancedQueueManager.registerWorker(
        "EMAIL_QUEUE",
        processConflictResolution,
      ),
      enhancedQueueManager.registerWorker(
        "WORKFLOW_CLEANUP",
        processDataValidation,
      ),
      enhancedQueueManager.registerWorker(
        "DEAD_LETTER",
        processFailedMigrationJobs,
      ),
    ]);

    console.log("‚úÖ All migration workers initialized successfully");
  } catch (error) {
    console.error("‚ùå Failed to initialize migration workers:", error);
    throw error;
  }
}

/**
 * Process migration data analysis with AI
 */
async function processMigrationAnalysis(
  job: Job<MigrationJobData>,
): Promise<any> {
  const { migrationJobId, organizationId, analysisType } = job.data;

  try {
    await job.updateProgress(10);

    console.log(`Starting AI analysis for migration job ${migrationJobId}`);

    // Update job status
    await updateMigrationStatus(
      migrationJobId,
      "analyzing",
      "Starting AI analysis...",
    );

    // Get migration job and files
    const { data: migrationJob, error: jobError } = await supabaseAdmin
      .from("migration_jobs")
      .select(
        `
        *,
        migration_files(*)
      `,
      )
      .eq("id", migrationJobId)
      .single();

    if (jobError || !migrationJob) {
      throw new Error(`Migration job not found: ${jobError?.message}`);
    }

    await job.updateProgress(20);

    // Process each uploaded file
    let totalProgress = 20;
    const progressPerFile = 60 / migrationJob.migration_files.length;

    for (const file of migrationJob.migration_files) {
      console.log(`Analyzing file: ${file.filename}`);

      // Download and parse file
      const fileData = await downloadAndParseFile(file);

      // Run AI analysis
      const analysis = await analyzeDataWithAI(
        fileData,
        analysisType || "goteamup",
      );

      // Store analysis results
      await storeAnalysisResults(migrationJobId, file.id, analysis);

      totalProgress += progressPerFile;
      await job.updateProgress(Math.round(totalProgress));
    }

    await job.updateProgress(90);

    // Generate final recommendations
    const recommendations =
      await generateMigrationRecommendations(migrationJobId);

    // Update job with AI analysis results
    await supabaseAdmin
      .from("migration_jobs")
      .update({
        ai_analysis: recommendations,
        status: "processed",
      })
      .eq("id", migrationJobId);

    await job.updateProgress(100);

    console.log(`AI analysis completed for migration job ${migrationJobId}`);

    // Queue next step: data validation
    await enhancedQueueManager.addJob(
      "WORKFLOW_CLEANUP",
      "validate-migration-data",
      { migrationJobId, organizationId },
      { priority: 4 },
    );

    return {
      status: "completed",
      recordsAnalyzed: recommendations.totalRecords,
      mappingsGenerated: recommendations.mappingCount,
      issuesFound: recommendations.issueCount,
    };
  } catch (error) {
    console.error(
      `Migration analysis failed for job ${migrationJobId}:`,
      error,
    );

    await updateMigrationStatus(
      migrationJobId,
      "failed",
      `Analysis failed: ${error.message}`,
    );

    throw error;
  }
}

/**
 * Process data import with conflict resolution
 */
async function processMigrationImport(
  job: Job<MigrationJobData>,
): Promise<any> {
  const { migrationJobId, organizationId, batchSize = 100 } = job.data;

  try {
    console.log(`Starting data import for migration job ${migrationJobId}`);

    await updateMigrationStatus(
      migrationJobId,
      "importing",
      "Starting data import...",
    );

    // Get migration records to process
    const { data: records, error } = await supabaseAdmin
      .from("migration_records")
      .select("*")
      .eq("migration_job_id", migrationJobId)
      .eq("status", "pending")
      .order("source_row_number")
      .limit(batchSize);

    if (error) {
      throw new Error(`Failed to get migration records: ${error.message}`);
    }

    if (!records || records.length === 0) {
      // Check if all records are processed
      const { data: jobStats } = await supabaseAdmin
        .from("migration_dashboard")
        .select("*")
        .eq("id", migrationJobId)
        .single();

      if (jobStats && jobStats.processed_records >= jobStats.total_records) {
        await completeMigrationJob(migrationJobId);
        return { status: "completed", message: "All records processed" };
      }

      return { status: "no_records", message: "No pending records to process" };
    }

    let processedCount = 0;
    let successCount = 0;
    let errorCount = 0;

    // Process records in smaller batches
    const microBatchSize = 10;
    for (let i = 0; i < records.length; i += microBatchSize) {
      const batch = records.slice(i, i + microBatchSize);

      for (const record of batch) {
        try {
          const result = await importSingleRecord(record, organizationId);

          if (result.success) {
            successCount++;
            await updateRecordStatus(
              record.id,
              "imported",
              result.targetRecordId,
            );
          } else if (result.conflict) {
            await createMigrationConflict(record, result.conflict);
            await updateRecordStatus(record.id, "pending");
          } else {
            errorCount++;
            await updateRecordStatus(record.id, "failed", null, result.errors);
          }

          processedCount++;
        } catch (recordError) {
          console.error(`Error processing record ${record.id}:`, recordError);
          errorCount++;
          await updateRecordStatus(record.id, "failed", null, [
            recordError.message,
          ]);
          processedCount++;
        }
      }

      // Update progress
      const progress = Math.round((processedCount / records.length) * 100);
      await job.updateProgress(progress);

      // Update job statistics
      await updateMigrationProgress(
        migrationJobId,
        processedCount,
        successCount,
        errorCount,
      );
    }

    // Check if more records need processing
    const { data: remainingRecords } = await supabaseAdmin
      .from("migration_records")
      .select("id")
      .eq("migration_job_id", migrationJobId)
      .eq("status", "pending")
      .limit(1);

    if (remainingRecords && remainingRecords.length > 0) {
      // Queue next batch
      await enhancedQueueManager.addJob(
        "WORKFLOW_ACTIONS",
        "import-migration-records",
        { migrationJobId, organizationId, batchSize },
        { priority: 4, delay: 5000 }, // 5 second delay to prevent overwhelming
      );
    } else {
      // Check for conflicts
      const { data: conflicts } = await supabaseAdmin
        .from("migration_conflicts")
        .select("id")
        .eq("migration_job_id", migrationJobId)
        .eq("resolution_status", "pending")
        .limit(1);

      if (!conflicts || conflicts.length === 0) {
        await completeMigrationJob(migrationJobId);
      }
    }

    return {
      status: "batch_completed",
      processedCount,
      successCount,
      errorCount,
    };
  } catch (error) {
    console.error(`Migration import failed for job ${migrationJobId}:`, error);

    await updateMigrationStatus(
      migrationJobId,
      "failed",
      `Import failed: ${error.message}`,
    );

    throw error;
  }
}

/**
 * Process conflict resolution
 */
async function processConflictResolution(
  job: Job<ConflictResolutionData>,
): Promise<any> {
  const { migrationJobId, conflictIds, resolution } = job.data;

  try {
    console.log(
      `Resolving ${conflictIds.length} conflicts for migration job ${migrationJobId}`,
    );

    let resolvedCount = 0;

    for (const conflictId of conflictIds) {
      try {
        await resolveConflict(conflictId, resolution);
        resolvedCount++;
      } catch (error) {
        console.error(`Failed to resolve conflict ${conflictId}:`, error);
      }

      const progress = Math.round((resolvedCount / conflictIds.length) * 100);
      await job.updateProgress(progress);
    }

    // Check if all conflicts are resolved
    const { data: remainingConflicts } = await supabaseAdmin
      .from("migration_conflicts")
      .select("id")
      .eq("migration_job_id", migrationJobId)
      .eq("resolution_status", "pending")
      .limit(1);

    if (!remainingConflicts || remainingConflicts.length === 0) {
      // Resume import process
      await enhancedQueueManager.addJob(
        "WORKFLOW_ACTIONS",
        "import-migration-records",
        { migrationJobId, organizationId: job.data.organizationId },
        { priority: 4 },
      );
    }

    return {
      status: "completed",
      resolvedCount,
    };
  } catch (error) {
    console.error(
      `Conflict resolution failed for job ${migrationJobId}:`,
      error,
    );
    throw error;
  }
}

/**
 * Process data validation
 */
async function processDataValidation(job: Job<MigrationJobData>): Promise<any> {
  const { migrationJobId, organizationId } = job.data;

  try {
    console.log(`Starting data validation for migration job ${migrationJobId}`);

    await updateMigrationStatus(
      migrationJobId,
      "processing",
      "Validating data...",
    );

    // Get field mappings for validation
    const { data: mappings, error } = await supabaseAdmin
      .from("migration_field_mappings")
      .select("*")
      .eq("migration_job_id", migrationJobId)
      .eq("is_active", true);

    if (error) {
      throw new Error(`Failed to get field mappings: ${error.message}`);
    }

    // Validate each mapping and update confidence scores
    let validationProgress = 0;
    for (const mapping of mappings) {
      const validationResult = await validateFieldMapping(mapping);

      await supabaseAdmin
        .from("migration_field_mappings")
        .update({
          is_validated: true,
          validation_errors: validationResult.errors,
          ai_confidence: validationResult.confidence,
        })
        .eq("id", mapping.id);

      validationProgress += 1;
      await job.updateProgress(
        Math.round((validationProgress / mappings.length) * 100),
      );
    }

    // Start the actual import process
    await enhancedQueueManager.addJob(
      "WORKFLOW_ACTIONS",
      "import-migration-records",
      { migrationJobId, organizationId },
      { priority: 4 },
    );

    return {
      status: "completed",
      mappingsValidated: mappings.length,
    };
  } catch (error) {
    console.error(`Data validation failed for job ${migrationJobId}:`, error);

    await updateMigrationStatus(
      migrationJobId,
      "failed",
      `Validation failed: ${error.message}`,
    );

    throw error;
  }
}

/**
 * Process failed migration jobs from dead letter queue
 */
async function processFailedMigrationJobs(job: Job<any>): Promise<any> {
  const originalJobData = job.data;

  try {
    console.log(
      "Processing failed migration job from dead letter queue:",
      originalJobData,
    );

    // Attempt to diagnose and fix the issue
    const diagnosis = await diagnoseMigrationFailure(originalJobData);

    if (diagnosis.canRetry) {
      // Retry the original job with fixed parameters
      await enhancedQueueManager.addJob(
        originalJobData.originalQueue,
        originalJobData.jobName,
        diagnosis.fixedData || originalJobData.data,
        { priority: 2, attempts: 1 },
      );

      return { status: "retried", diagnosis: diagnosis.issue };
    } else {
      // Mark as permanently failed and notify user
      if (originalJobData.data?.migrationJobId) {
        await updateMigrationStatus(
          originalJobData.data.migrationJobId,
          "failed",
          `Permanent failure: ${diagnosis.issue}`,
        );
      }

      return { status: "permanently_failed", diagnosis: diagnosis.issue };
    }
  } catch (error) {
    console.error("Failed to process dead letter queue job:", error);
    return { status: "dead_letter_failed", error: error.message };
  }
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

async function downloadAndParseFile(file: any): Promise<any[]> {
  // Download file from storage
  const { data: fileContent, error } = await supabaseAdmin.storage
    .from("migration-uploads")
    .download(file.storage_path);

  if (error) {
    throw new Error(`Failed to download file: ${error.message}`);
  }

  // Parse based on file type
  const text = await fileContent.text();

  if (file.file_type === "csv") {
    const lines = text.split("\n").filter((line) => line.trim());
    const headers = lines[0]
      .split(",")
      .map((h) => h.trim().replace(/['"]/g, ""));

    return lines.slice(1).map((line, index) => {
      const values = line.split(",").map((v) => v.trim().replace(/['"]/g, ""));
      const record: any = { _rowNumber: index + 2 };

      headers.forEach((header, i) => {
        record[header] = values[i] || "";
      });

      return record;
    });
  }

  throw new Error(`File type ${file.file_type} not supported yet`);
}

async function storeAnalysisResults(
  migrationJobId: string,
  fileId: string,
  analysis: any,
): Promise<void> {
  // Update file with analysis results
  await supabaseAdmin
    .from("migration_files")
    .update({
      detected_data_types: analysis.dataTypes,
      column_analysis: analysis.columns,
      data_quality_score: analysis.qualityScore,
      total_rows: analysis.columns[0]?.sampleValues?.length || 0,
      valid_rows: analysis.validRows,
      processing_status: "processed",
    })
    .eq("id", fileId);

  // Store field mappings
  if (analysis.fieldMappings?.length > 0) {
    const mappingRecords = analysis.fieldMappings.map((mapping) => ({
      migration_job_id: migrationJobId,
      organization_id: mapping.organizationId,
      source_field: mapping.sourceField,
      source_column_index: 0, // Would need to calculate from actual data
      source_data_type: mapping.sourceDataType,
      source_sample_values: mapping.sampleValues,
      target_table: mapping.targetTable,
      target_field: mapping.targetField,
      target_data_type: mapping.targetDataType,
      transformation_type: mapping.transformationType,
      transformation_config: mapping.transformationConfig || {},
      ai_confidence: mapping.confidence,
      is_required: mapping.isRequired,
      mapping_priority: mapping.priority,
    }));

    await supabaseAdmin.from("migration_field_mappings").insert(mappingRecords);
  }
}

async function generateMigrationRecommendations(
  migrationJobId: string,
): Promise<any> {
  const { data: files } = await supabaseAdmin
    .from("migration_files")
    .select("*")
    .eq("migration_job_id", migrationJobId);

  const { data: mappings } = await supabaseAdmin
    .from("migration_field_mappings")
    .select("*")
    .eq("migration_job_id", migrationJobId);

  return {
    totalRecords:
      files?.reduce((sum, file) => sum + (file.total_rows || 0), 0) || 0,
    mappingCount: mappings?.length || 0,
    issueCount:
      files?.filter((file) => (file.data_quality_score || 0) < 0.8).length || 0,
    averageQuality:
      files?.reduce((sum, file) => sum + (file.data_quality_score || 0), 0) /
        (files?.length || 1) || 0,
    recommendations: [
      "Review field mappings for accuracy",
      "Resolve data quality issues before import",
      "Test import with small batch first",
    ],
  };
}

async function importSingleRecord(
  record: any,
  organizationId: string,
): Promise<any> {
  try {
    // Get field mappings for this migration
    const { data: mappings } = await supabaseAdmin
      .from("migration_field_mappings")
      .select("*")
      .eq("migration_job_id", record.migration_job_id)
      .eq("is_active", true)
      .order("mapping_priority");

    if (!mappings || mappings.length === 0) {
      return {
        success: false,
        errors: ["No field mappings found"],
      };
    }

    // Transform source data using mappings
    const transformedData = await transformRecordData(
      record.source_data,
      mappings,
      organizationId,
    );

    // Check for duplicates
    const duplicateCheck = await checkForDuplicates(transformedData, mappings);

    if (duplicateCheck.isDuplicate) {
      return {
        success: false,
        conflict: {
          type: "duplicate_record",
          existingRecord: duplicateCheck.existingRecord,
          newRecord: transformedData,
        },
      };
    }

    // Import the record
    const importResult = await performRecordImport(transformedData, mappings);

    return {
      success: true,
      targetRecordId: importResult.id,
    };
  } catch (error) {
    return {
      success: false,
      errors: [error.message],
    };
  }
}

async function transformRecordData(
  sourceData: any,
  mappings: any[],
  organizationId: string,
): Promise<any> {
  const transformedData = { organization_id: organizationId };

  for (const mapping of mappings) {
    const sourceValue = sourceData[mapping.source_field];

    if (sourceValue != null && sourceValue !== "") {
      let transformedValue = sourceValue;

      switch (mapping.transformation_type) {
        case "format":
          transformedValue = formatValue(sourceValue, mapping.target_data_type);
          break;
        case "lookup":
          transformedValue = await lookupValue(
            sourceValue,
            mapping.transformation_config,
          );
          break;
        // Add more transformation types as needed
      }

      transformedData[mapping.target_field] = transformedValue;
    }
  }

  return transformedData;
}

async function checkForDuplicates(data: any, mappings: any[]): Promise<any> {
  // Simple email-based duplicate check for clients
  const clientMapping = mappings.find((m) => m.target_table === "clients");

  if (clientMapping && data.email) {
    const { data: existingClient } = await supabaseAdmin
      .from("clients")
      .select("*")
      .eq("email", data.email)
      .eq("organization_id", data.organization_id)
      .limit(1);

    if (existingClient && existingClient.length > 0) {
      return {
        isDuplicate: true,
        existingRecord: existingClient[0],
      };
    }
  }

  return { isDuplicate: false };
}

async function performRecordImport(data: any, mappings: any[]): Promise<any> {
  // Group data by target table
  const tableData = {};

  for (const mapping of mappings) {
    if (!tableData[mapping.target_table]) {
      tableData[mapping.target_table] = { ...data };
    }
  }

  // Import to primary table first (usually clients)
  const primaryTable = Object.keys(tableData)[0];
  const { data: insertedRecord, error } = await supabaseAdmin
    .from(primaryTable)
    .insert(tableData[primaryTable])
    .select()
    .single();

  if (error) {
    throw new Error(`Import failed: ${error.message}`);
  }

  return insertedRecord;
}

function formatValue(value: any, targetType: string): any {
  switch (targetType) {
    case "date":
      return new Date(value).toISOString().split("T")[0];
    case "phone":
      return value.toString().replace(/\D/g, "");
    case "email":
      return value.toString().toLowerCase();
    default:
      return value;
  }
}

async function lookupValue(value: any, config: any): Promise<any> {
  // Implement lookup logic based on config
  // For now, return original value
  return value;
}

async function updateMigrationStatus(
  jobId: string,
  status: string,
  message?: string,
): Promise<void> {
  const updates: any = { status };

  if (message) {
    // Log the status change
    await supabaseAdmin.from("migration_logs").insert({
      migration_job_id: jobId,
      log_level: "info",
      message: message,
      step: status,
    });
  }

  await supabaseAdmin.from("migration_jobs").update(updates).eq("id", jobId);
}

async function updateMigrationProgress(
  jobId: string,
  processed: number,
  success: number,
  failed: number,
): Promise<void> {
  await supabaseAdmin
    .from("migration_jobs")
    .update({
      processed_records: processed,
      successful_imports: success,
      failed_imports: failed,
    })
    .eq("id", jobId);
}

async function updateRecordStatus(
  recordId: string,
  status: string,
  targetRecordId?: string,
  errors?: string[],
): Promise<void> {
  const updates: any = {
    status,
    processed_at: new Date().toISOString(),
  };

  if (targetRecordId) {
    updates.target_record_id = targetRecordId;
  }

  if (errors) {
    updates.processing_errors = errors;
  }

  await supabaseAdmin
    .from("migration_records")
    .update(updates)
    .eq("id", recordId);
}

async function createMigrationConflict(
  record: any,
  conflict: any,
): Promise<void> {
  await supabaseAdmin.from("migration_conflicts").insert({
    migration_job_id: record.migration_job_id,
    migration_record_id: record.id,
    organization_id: record.organization_id,
    conflict_type: conflict.type,
    conflict_description: `Duplicate record found`,
    existing_data: conflict.existingRecord,
    new_data: conflict.newRecord,
    resolution_status: "pending",
  });
}

async function resolveConflict(
  conflictId: string,
  resolution: any,
): Promise<void> {
  await supabaseAdmin
    .from("migration_conflicts")
    .update({
      resolution_status: "resolved",
      resolution_action: resolution.action,
      resolution_data: resolution.data,
      resolved_at: new Date().toISOString(),
    })
    .eq("id", conflictId);
}

async function validateFieldMapping(mapping: any): Promise<any> {
  // Perform validation checks on field mappings
  const errors = [];
  let confidence = mapping.ai_confidence || 0.5;

  // Check if target field exists in schema
  // Check data type compatibility
  // Check for required field constraints

  return {
    errors,
    confidence: Math.min(confidence + 0.1, 1.0), // Slight confidence boost for validation
  };
}

async function completeMigrationJob(migrationJobId: string): Promise<void> {
  await supabaseAdmin
    .from("migration_jobs")
    .update({
      status: "completed",
      completed_at: new Date().toISOString(),
    })
    .eq("id", migrationJobId);

  console.log(`Migration job ${migrationJobId} completed successfully`);
}

async function diagnoseMigrationFailure(failedJobData: any): Promise<any> {
  // Analyze the failure and determine if it can be retried
  const issue = failedJobData.error || "Unknown error";

  const retryableErrors = ["timeout", "connection", "rate limit", "temporary"];

  const canRetry = retryableErrors.some((errorType) =>
    issue.toLowerCase().includes(errorType),
  );

  return {
    canRetry,
    issue,
    fixedData: canRetry ? failedJobData.data : null,
  };
}
